{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9208aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# # ============================\n",
    "# Name:Delight Mapololo\n",
    "# Reg No. : R229476R\n",
    "# Date: Nov 27, 2025\n",
    "# Project: House Price Prediction using 21 CNN models\n",
    "# ============================\n",
    "# ---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db63f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7295f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active computation device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Active computation device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6305ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset from: C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\final_zimbabwe_property_listings_complete.csv\n",
      "Imported rows: 1613\n",
      "Image folder set to: C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\images\n",
      "Using CSV image column: image_filenames\n",
      "Resolved image paths: 468 / 1613 rows\n",
      "Detected price column: price\n",
      "Rows after dropping NaN prices: 1610\n",
      "Price normalizer (StandardScaler) initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Load CSV + Image Folder\n",
    "# -----------------------------\n",
    "# Adjust this path to your CSV file\n",
    "csv_path = r\"C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\final_zimbabwe_property_listings_complete.csv\"\n",
    "print(f\"Reading dataset from: {csv_path}\")\n",
    "property_data = pd.read_csv(csv_path)\n",
    "print(f\"Imported rows: {len(property_data)}\")\n",
    "\n",
    "# The image folder you originally used in your code:\n",
    "image_folder = Path(r\"C:\\Users\\Brendon\\Desktop\\Deep_Learning\\ASSIGNMENTS\\images\")\n",
    "print(f\"Image folder set to: {image_folder}\")\n",
    "\n",
    "# Attempt to detect image filename column in CSV\n",
    "possible_image_cols = ['image_filenames', 'image_filename', 'image_path', 'filename', 'image', 'images']\n",
    "image_col = None\n",
    "for col in property_data.columns:\n",
    "    if col.lower() in [c.lower() for c in possible_image_cols]:\n",
    "        image_col = col\n",
    "        break\n",
    "if image_col is None:\n",
    "    # fallback: any column containing 'image' or 'photo'\n",
    "    for col in property_data.columns:\n",
    "        if 'image' in col.lower() or 'photo' in col.lower():\n",
    "            image_col = col\n",
    "            break\n",
    "if image_col is None:\n",
    "    raise KeyError(\"No image column detected in the CSV. Expected columns like image_filenames or image_path.\")\n",
    "print(f\"Using CSV image column: {image_col}\")\n",
    "\n",
    "# Helper to resolve image path (your images are in the folder so prefer folder join)\n",
    "def resolve_image_path_from_folder(cell_value):\n",
    "    \"\"\"\n",
    "    If cell_value already contains a full path, prefer it (but we'll prefer images in image_folder).\n",
    "    If a filename is provided, join with image_folder and check existence.\n",
    "    If multiple filenames are present, take the first (split on '|' or ';').\n",
    "    \"\"\"\n",
    "    if pd.isna(cell_value):\n",
    "        return None\n",
    "\n",
    "    if isinstance(cell_value, str):\n",
    "        # pick first filename if multiple\n",
    "        if '|' in cell_value:\n",
    "            candidate = cell_value.split('|')[0].strip()\n",
    "        elif ';' in cell_value:\n",
    "            candidate = cell_value.split(';')[0].strip()\n",
    "        else:\n",
    "            candidate = cell_value.strip()\n",
    "    else:\n",
    "        candidate = str(cell_value)\n",
    "\n",
    "    # If candidate looks like an absolute path and exists, return it\n",
    "    cand_path = Path(candidate)\n",
    "    if cand_path.exists():\n",
    "        return cand_path\n",
    "\n",
    "    # Otherwise, try joining with provided image_folder\n",
    "    joined = image_folder / candidate\n",
    "    if joined.exists():\n",
    "        return joined\n",
    "\n",
    "    # Try common extensions appended to candidate while joined to folder\n",
    "    for ext in ['.jpg', '.jpeg', '.png', '.webp', '.bmp']:\n",
    "        p = image_folder / (candidate + ext)\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # Not found\n",
    "    return None\n",
    "\n",
    "# Create 'image_location' column with resolved Path or None\n",
    "property_data['image_location'] = property_data[image_col].apply(resolve_image_path_from_folder)\n",
    "valid_images = property_data['image_location'].notna().sum()\n",
    "print(f\"Resolved image paths: {valid_images} / {len(property_data)} rows\")\n",
    "\n",
    "# Detect price column\n",
    "price_col = None\n",
    "for col in property_data.columns:\n",
    "    if 'price' in col.lower() or 'value' in col.lower() or 'listing' in col.lower():\n",
    "        price_col = col\n",
    "        break\n",
    "if price_col is None:\n",
    "    raise KeyError(\"No price-like column found in CSV.\")\n",
    "print(f\"Detected price column: {price_col}\")\n",
    "\n",
    "# Convert price to numeric and drop rows without price\n",
    "property_data[price_col] = pd.to_numeric(property_data[price_col], errors='coerce')\n",
    "property_data = property_data[~property_data[price_col].isna()].reset_index(drop=True)\n",
    "print(f\"Rows after dropping NaN prices: {len(property_data)}\")\n",
    "\n",
    "# Fit price normalizer (StandardScaler)\n",
    "price_normalizer = StandardScaler()\n",
    "price_normalizer.fit(property_data[[price_col]].values)\n",
    "print(\"Price normalizer (StandardScaler) initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312c1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Dataset Class\n",
    "# -----------------------------\n",
    "class PropertyValueDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_column, image_folder_path, preprocessing=None,\n",
    "                 incorporate_features=True, value_normalizer=None):\n",
    "        self.df = dataframe.reset_index(drop=True).copy()\n",
    "        self.image_column = image_column\n",
    "        self.image_folder = Path(image_folder_path)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.incorporate_features = incorporate_features\n",
    "        self.value_normalizer = value_normalizer\n",
    "\n",
    "        # Numerical attributes detection (flexible)\n",
    "        potential_numerical_cols = [\n",
    "            'building_area(mÂ²)', 'land_area(mÂ²)', 'bedrooms', 'bathrooms', 'rooms',\n",
    "            'area', 'size', 'building_area', 'land_area', 'square_feet', 'sqft',\n",
    "            'bed', 'bath', 'room_count'\n",
    "        ]\n",
    "        self.numerical_attributes = [c for c in potential_numerical_cols if c in self.df.columns]\n",
    "        if not self.numerical_attributes:\n",
    "            self.df['dummy_feature'] = 1.0\n",
    "            self.numerical_attributes = ['dummy_feature']\n",
    "            print(\"âš ï¸ No numeric columns found -> added dummy_feature\")\n",
    "\n",
    "        # Geo encoding\n",
    "        potential_location_cols = ['location', 'city', 'region', 'area', 'neighborhood', 'suburb', 'town']\n",
    "        location_col = None\n",
    "        for c in potential_location_cols:\n",
    "            if c in self.df.columns:\n",
    "                location_col = c\n",
    "                break\n",
    "        if location_col:\n",
    "            self.geo_encoder = LabelEncoder()\n",
    "            self.df['geo_encoded'] = self.geo_encoder.fit_transform(self.df[location_col].fillna('Unspecified').astype(str))\n",
    "            print(f\"Using location column '{location_col}' for geo encoding.\")\n",
    "        else:\n",
    "            self.df['geo_encoded'] = 0\n",
    "            print(\"No location column found -> using geo_encoded = 0\")\n",
    "\n",
    "        # Feature columns and scaler\n",
    "        self.feature_columns = [c for c in self.numerical_attributes if c in self.df.columns] + ['geo_encoded']\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        feature_matrix = self.df[self.feature_columns].fillna(0).astype(float).values\n",
    "        self.feature_matrix = self.feature_scaler.fit_transform(feature_matrix)\n",
    "        print(f\"Feature matrix prepared with shape: {self.feature_matrix.shape}\")\n",
    "\n",
    "        # store price column name for target extraction (outer scope price_col used)\n",
    "        self.price_column = price_col\n",
    "\n",
    "        # Ensure image_location column exists and contains Path objects when resolvable\n",
    "        if 'image_location' not in self.df.columns:\n",
    "            self.df['image_location'] = None\n",
    "        else:\n",
    "            # keep Path objects or None\n",
    "            def to_path(x):\n",
    "                try:\n",
    "                    if pd.isna(x):\n",
    "                        return None\n",
    "                    p = Path(x)\n",
    "                    return p if p.exists() else None\n",
    "                except Exception:\n",
    "                    return None\n",
    "            self.df['image_location'] = self.df['image_location'].apply(to_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Resolve image path (prefer precomputed image_location)\n",
    "        img_path = None\n",
    "        if row.get('image_location', None) is not None:\n",
    "            img_path = row['image_location']\n",
    "        else:\n",
    "            raw = row[self.image_column]\n",
    "            if isinstance(raw, str):\n",
    "                cand = Path(raw)\n",
    "                if cand.exists():\n",
    "                    img_path = cand\n",
    "                else:\n",
    "                    joined = self.image_folder / raw\n",
    "                    if joined.exists():\n",
    "                        img_path = joined\n",
    "                    else:\n",
    "                        # try first part if cell contains multiple filenames\n",
    "                        if '|' in raw:\n",
    "                            candidate = raw.split('|')[0].strip()\n",
    "                            joined2 = self.image_folder / candidate\n",
    "                            if joined2.exists():\n",
    "                                img_path = joined2\n",
    "            else:\n",
    "                img_path = None\n",
    "\n",
    "        # Load image or fallback blank\n",
    "        if img_path is not None:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "            except Exception:\n",
    "                img = Image.new('RGB', (224, 224), color='black')\n",
    "        else:\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "        if self.preprocessing is not None:\n",
    "            img = self.preprocessing(img)\n",
    "\n",
    "        # attribute features\n",
    "        attributes = torch.tensor(self.feature_matrix[idx], dtype=torch.float32)\n",
    "\n",
    "        # target (price), possibly normalized\n",
    "        raw_price = float(row[self.price_column])\n",
    "        if self.value_normalizer is not None:\n",
    "            price_val = float(self.value_normalizer.transform([[raw_price]])[0][0])\n",
    "        else:\n",
    "            price_val = raw_price\n",
    "        price_tensor = torch.tensor(price_val, dtype=torch.float32)\n",
    "\n",
    "        if self.incorporate_features:\n",
    "            return img, attributes, price_tensor\n",
    "        else:\n",
    "            return img, price_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0bddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test counts: 1127, 241, 242\n",
      "Using location column 'location' for geo encoding.\n",
      "Feature matrix prepared with shape: (1127, 5)\n",
      "Using location column 'location' for geo encoding.\n",
      "Feature matrix prepared with shape: (241, 5)\n",
      "Using location column 'location' for geo encoding.\n",
      "Feature matrix prepared with shape: (242, 5)\n",
      "DataLoaders created â€” train batches: 71, val: 16, test: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Transforms and Dataloaders\n",
    "# -----------------------------\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Split data\n",
    "train_df, temp_df = train_test_split(property_data, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "print(f\"Train/Val/Test counts: {len(train_df)}, {len(val_df)}, {len(test_df)}\")\n",
    "\n",
    "# Create datasets (they will resolve images using the provided image_folder)\n",
    "train_dataset = PropertyValueDataset(train_df, image_column=image_col, image_folder_path=image_folder,\n",
    "                                    preprocessing=train_transforms, incorporate_features=True, value_normalizer=price_normalizer)\n",
    "val_dataset = PropertyValueDataset(val_df, image_column=image_col, image_folder_path=image_folder,\n",
    "                                   preprocessing=val_transforms, incorporate_features=True, value_normalizer=price_normalizer)\n",
    "test_dataset = PropertyValueDataset(test_df, image_column=image_col, image_folder_path=image_folder,\n",
    "                                    preprocessing=val_transforms, incorporate_features=True, value_normalizer=price_normalizer)\n",
    "\n",
    "# DataLoader settings (Windows-friendly defaults)\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0  # set to >0 on Linux for speed\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(f\"DataLoaders created â€” train batches: {len(train_loader)}, val: {len(val_loader)}, test: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fa60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 5: Model + Multimodal Network\n",
    "# -----------------------------\n",
    "class PropertyValuationNetwork(nn.Module):\n",
    "    def __init__(self, base_architecture, feature_count=7, dropout_rate=0.5, architecture_variant='standard'):\n",
    "        super().__init__()\n",
    "        self.architecture_variant = architecture_variant\n",
    "        self.base = base_architecture\n",
    "\n",
    "        # Try to neutralize final classifier to extract embeddings\n",
    "        # Different model families have different attribute names for final layers.\n",
    "        # We'll handle common cases; fallback to identity and a sample forward to infer dim.\n",
    "        try:\n",
    "            if architecture_variant in ['vgg','alexnet']:\n",
    "                # keep features & classifier trimmed\n",
    "                self.base.classifier = nn.Sequential(*list(self.base.classifier.children())[:-1])\n",
    "            elif architecture_variant == 'densenet':\n",
    "                # densenet.classifier exists\n",
    "                _ = None\n",
    "            elif architecture_variant == 'inception':\n",
    "                self.base.fc = nn.Identity()\n",
    "                self.base.aux_logits = False\n",
    "            else:\n",
    "                # For resnet-like models, replace fc with identity\n",
    "                if hasattr(self.base, 'fc'):\n",
    "                    self.base.fc = nn.Identity()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Infer visual_feature_dim with a sample forward\n",
    "        with torch.no_grad():\n",
    "            self.base.eval()\n",
    "            sample_input = torch.zeros(1,3,224,224)\n",
    "            try:\n",
    "                sample_out = self.base(sample_input)\n",
    "                visual_feature_dim = sample_out.view(1, -1).size(1)\n",
    "            except Exception:\n",
    "                # As a safe fallback\n",
    "                visual_feature_dim = 2048\n",
    "\n",
    "        # Multimodal head\n",
    "        self.feature_integration = nn.Sequential(\n",
    "            nn.Linear(visual_feature_dim + feature_count, 1536),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # weight init for linear layers\n",
    "        for m in self.feature_integration.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, visual_input, attribute_features):\n",
    "        # Get visual embedding\n",
    "        if self.architecture_variant in ['vgg','alexnet']:\n",
    "            visual_features = self.base(visual_input)\n",
    "            visual_features = visual_features.view(visual_features.size(0), -1)\n",
    "        elif self.architecture_variant == 'densenet':\n",
    "            # densenet: use features->relu->adaptive_avg_pool2d->flatten\n",
    "            feats = self.base.features(visual_input)\n",
    "            feats = nn.functional.relu(feats, inplace=True)\n",
    "            feats = nn.functional.adaptive_avg_pool2d(feats, (1,1))\n",
    "            visual_features = torch.flatten(feats, 1)\n",
    "        else:\n",
    "            out = self.base(visual_input)\n",
    "            visual_features = out.view(out.size(0), -1)\n",
    "\n",
    "        fused = torch.cat([visual_features, attribute_features], dim=1)\n",
    "        out = self.feature_integration(fused)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "# Factory to initialize backbones and wrap into PropertyValuationNetwork\n",
    "def initialize_neural_network(model_identifier, feature_dimensions=7):\n",
    "    mapping = {\n",
    "        'EfficientNet': (lambda: models.efficientnet_b0(pretrained=True), 'efficientnet'),\n",
    "        'MobileNet-v2': (lambda: models.mobilenet_v2(pretrained=True), 'mobilenet'),\n",
    "        'ResNet': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'DenseNet': (lambda: models.densenet121(pretrained=True), 'densenet'),\n",
    "        'Xception': (lambda: models.resnet50(pretrained=True), 'standard'),  # substitute (torchvision lacks xception)\n",
    "        'Inception-V3': (lambda: models.inception_v3(pretrained=True, aux_logits=True), 'inception'),\n",
    "        'GoogleNet': (lambda: models.googlenet(pretrained=True, aux_logits=True), 'googlenet'),\n",
    "        'VGG': (lambda: models.vgg16(pretrained=True), 'vgg'),\n",
    "        'AlexNet': (lambda: models.alexnet(pretrained=True), 'alexnet'),\n",
    "        'NIN': (lambda: models.vgg16(pretrained=True), 'vgg'),\n",
    "        'ZFNet': (lambda: models.alexnet(pretrained=True), 'alexnet'),\n",
    "        # many architectures not directly in torchvision are substituted with resnet50 as a backbone\n",
    "        'Squeeze-and-Excitation': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'Residual-Attention': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'WideResNet': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'Inception-ResNet-v2': (lambda: models.inception_v3(pretrained=True, aux_logits=True), 'inception'),\n",
    "        'Inception-V4': (lambda: models.inception_v3(pretrained=True, aux_logits=True), 'inception'),\n",
    "        'Competitive-SE': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'HRNetV2': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'FractalNet': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'Highway': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "        'CapsuleNet': (lambda: models.resnet50(pretrained=True), 'standard'),\n",
    "    }\n",
    "\n",
    "    if model_identifier not in mapping:\n",
    "        raise ValueError(f\"Unknown architecture: {model_identifier}\")\n",
    "\n",
    "    backbone_init, arch_variant = mapping[model_identifier]\n",
    "    backbone = backbone_init()\n",
    "    net = PropertyValuationNetwork(backbone, feature_count=feature_dimensions, architecture_variant=arch_variant)\n",
    "    return net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99da177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 6: Training Function\n",
    "# -----------------------------\n",
    "def execute_model_training(network_name, training_loader, validation_loader, training_cycles=7, learning_rate=0.003):\n",
    "    print(f\"\\n{'='*60}\\nStarting training for: {network_name}\\n{'='*60}\")\n",
    "    # determine feature_dimensions from loaders (datasets)\n",
    "    # Expect dataset returns (img, attributes, price)\n",
    "    if 'feature_dimensions' not in globals():\n",
    "        try:\n",
    "            sample = next(iter(training_loader))\n",
    "            # sample[1] should be attribute tensor\n",
    "            feature_dims = sample[1].shape[1]\n",
    "        except Exception:\n",
    "            # fallback to dataset.feature_matrix shape\n",
    "            feature_dims = getattr(training_loader.dataset, 'feature_matrix').shape[1]\n",
    "        globals()['feature_dimensions'] = feature_dims\n",
    "        print(f\"Feature dimensions set to: {feature_dims}\")\n",
    "    else:\n",
    "        feature_dims = globals()['feature_dimensions']\n",
    "        print(f\"Using existing feature_dimensions: {feature_dims}\")\n",
    "\n",
    "    # Build model\n",
    "    nn_start = time.time()\n",
    "    try:\n",
    "        model = initialize_neural_network(network_name, feature_dimensions=feature_dims)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize {network_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"Model initialized. Params:\", sum(p.numel() for p in model.parameters()))\n",
    "    # Loss and optimizer\n",
    "    l1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "    def composite_loss(pred, tgt):\n",
    "        return 0.5 * l1(pred, tgt) + 0.5 * mse(pred, tgt)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate * 0.7, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=6, min_lr=1e-6, cooldown=2)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    # adjust cycles for heavy nets\n",
    "    heavy = ['ResNet', 'VGG', 'Inception-V3', 'Inception-ResNet-v2', 'Inception-V4', 'DenseNet']\n",
    "    if network_name in heavy:\n",
    "        training_cycles = min(training_cycles, 7)\n",
    "        print(f\"Resource-intensive architecture -> cycles set to {training_cycles}\")\n",
    "\n",
    "    history = {'training_loss': [], 'validation_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience = 10\n",
    "    wait = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(training_cycles):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        n_batches = 0\n",
    "        loop = tqdm(training_loader, desc=f\"{network_name} Epoch {epoch+1}/{training_cycles}\", ncols=100)\n",
    "        for imgs, attrs, targets in loop:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            attrs = attrs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if grad_scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    preds = model(imgs, attrs)\n",
    "                    loss = composite_loss(preds, targets)\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "            else:\n",
    "                preds = model(imgs, attrs)\n",
    "                loss = composite_loss(preds, targets)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "            if n_batches % 10 == 0:\n",
    "                loop.set_postfix({'batch_loss': f\"{loss.item():.4f}\", 'avg_loss': f\"{train_loss/n_batches:.4f}\"})\n",
    "\n",
    "        avg_train_loss = train_loss / max(1, n_batches)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        preds_collect = []\n",
    "        truth_collect = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, attrs, targets in validation_loader:\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                attrs = attrs.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "                if grad_scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        preds = model(imgs, attrs)\n",
    "                else:\n",
    "                    preds = model(imgs, attrs)\n",
    "\n",
    "                loss = composite_loss(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "                preds_collect.extend(preds.cpu().numpy())\n",
    "                truth_collect.extend(targets.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / max(1, len(validation_loader))\n",
    "        # denormalize for metrics if needed\n",
    "        preds_arr = np.array(preds_collect).reshape(-1,1)\n",
    "        truth_arr = np.array(truth_collect).reshape(-1,1)\n",
    "        if hasattr(validation_loader.dataset, 'value_normalizer') and validation_loader.dataset.value_normalizer is not None:\n",
    "            den_preds = validation_loader.dataset.value_normalizer.inverse_transform(preds_arr).flatten()\n",
    "            den_truth = validation_loader.dataset.value_normalizer.inverse_transform(truth_arr).flatten()\n",
    "        else:\n",
    "            den_preds = preds_arr.flatten()\n",
    "            den_truth = truth_arr.flatten()\n",
    "\n",
    "        val_r2 = r2_score(den_truth, den_preds) if len(den_preds)>0 else float('nan')\n",
    "        val_rmse = np.sqrt(mean_squared_error(den_truth, den_preds)) if len(den_preds)>0 else float('nan')\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        history['training_loss'].append(avg_train_loss)\n",
    "        history['validation_loss'].append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{training_cycles} | TrainLoss={avg_train_loss:.6f} | ValLoss={avg_val_loss:.6f} | ValR2={val_r2:.4f} | ValRMSE={val_rmse:.2f}\")\n",
    "\n",
    "        # early stopping bookkeeping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    total_train_time = time.time() - t0\n",
    "\n",
    "    # restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Final evaluation on test_loader\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_truths = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, attrs, targets in test_loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            attrs = attrs.to(device, non_blocking=True)\n",
    "            preds = model(imgs, attrs)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_truths.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_truths = np.array(test_truths).reshape(-1,1)\n",
    "    if hasattr(test_loader.dataset, 'value_normalizer') and test_loader.dataset.value_normalizer is not None:\n",
    "        final_preds = test_loader.dataset.value_normalizer.inverse_transform(test_preds).flatten()\n",
    "        final_truths = test_loader.dataset.value_normalizer.inverse_transform(test_truths).flatten()\n",
    "    else:\n",
    "        final_preds = test_preds.flatten()\n",
    "        final_truths = test_truths.flatten()\n",
    "\n",
    "    eval_r2 = r2_score(final_truths, final_preds) if len(final_preds)>0 else float('nan')\n",
    "    eval_rmse = np.sqrt(mean_squared_error(final_truths, final_preds)) if len(final_preds)>0 else float('nan')\n",
    "    eval_mse = mean_squared_error(final_truths, final_preds) if len(final_preds)>0 else float('nan')\n",
    "    eval_mae = mean_absolute_error(final_truths, final_preds) if len(final_preds)>0 else float('nan')\n",
    "\n",
    "    metrics = {\n",
    "        'architecture_name': network_name,\n",
    "        'evaluation_r2': eval_r2,\n",
    "        'evaluation_rmse': eval_rmse,\n",
    "        'evaluation_mse': eval_mse,\n",
    "        'evaluation_mae': eval_mae,\n",
    "        'optimal_validation_loss': best_val_loss,\n",
    "        'training_duration': total_train_time,\n",
    "        'training_history': history,\n",
    "        'model_parameters': best_state\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{network_name} summary: R2={eval_r2:.4f}, RMSE={eval_rmse:.2f}, Training time={total_train_time:.2f}s\")\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b342dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set feature_dimensions = 5\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ Starting training of 21 models on cpu\n",
      "================================================================================\n",
      "\n",
      "[1/21] -> EfficientNet\n",
      "\n",
      "============================================================\n",
      "Starting training for: EfficientNet\n",
      "============================================================\n",
      "Using existing feature_dimensions: 5\n",
      "Model initialized. Params: 9761381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [04:24<00:00,  3.72s/it, batch_loss=0.0927, avg_loss=0.3387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | TrainLoss=0.337048 | ValLoss=3.682741 | ValR2=-0.1150 | ValRMSE=2249356373147.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [06:00<00:00,  5.07s/it, batch_loss=0.0246, avg_loss=0.1128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | TrainLoss=0.114801 | ValLoss=3.419084 | ValR2=-0.0482 | ValRMSE=2180941589773.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [04:52<00:00,  4.12s/it, batch_loss=0.0631, avg_loss=0.0879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | TrainLoss=0.088923 | ValLoss=3.292941 | ValR2=-0.0163 | ValRMSE=2147481836059.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:58<00:00,  2.51s/it, batch_loss=0.0440, avg_loss=0.0679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | TrainLoss=0.070789 | ValLoss=3.508451 | ValR2=-0.0717 | ValRMSE=2205156605062.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:57<00:00,  2.51s/it, batch_loss=0.0625, avg_loss=0.0605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | TrainLoss=0.060628 | ValLoss=3.496703 | ValR2=-0.0698 | ValRMSE=2203278846729.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [03:23<00:00,  2.87s/it, batch_loss=0.0520, avg_loss=0.0648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | TrainLoss=0.064777 | ValLoss=3.225539 | ValR2=-0.0040 | ValRMSE=2134457637297.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EfficientNet Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [03:22<00:00,  2.85s/it, batch_loss=0.0187, avg_loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | TrainLoss=0.056044 | ValLoss=3.229373 | ValR2=-0.0042 | ValRMSE=2134581529985.91\n",
      "\n",
      "EfficientNet summary: R2=-3851715.2500, RMSE=2168528054.98, Training time=1782.87s\n",
      "Saved best_EfficientNet.pth (elapsed 29.96 min)\n",
      "\n",
      "[2/21] -> MobileNet-v2\n",
      "\n",
      "============================================================\n",
      "Starting training for: MobileNet-v2\n",
      "============================================================\n",
      "Using existing feature_dimensions: 5\n",
      "Model initialized. Params: 7977705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:37<00:00,  2.22s/it, batch_loss=0.1193, avg_loss=0.2676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | TrainLoss=0.265264 | ValLoss=3.590641 | ValR2=-0.0816 | ValRMSE=2215404307256.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:37<00:00,  2.22s/it, batch_loss=0.0686, avg_loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | TrainLoss=0.093712 | ValLoss=3.228438 | ValR2=-0.0048 | ValRMSE=2135294836443.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:38<00:00,  2.24s/it, batch_loss=0.1264, avg_loss=0.0632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | TrainLoss=0.063345 | ValLoss=3.253137 | ValR2=-0.0082 | ValRMSE=2138847760860.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:35<00:00,  2.19s/it, batch_loss=0.0179, avg_loss=0.0635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | TrainLoss=0.063275 | ValLoss=3.245179 | ValR2=-0.0071 | ValRMSE=2137671457306.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:41<00:00,  2.28s/it, batch_loss=0.0793, avg_loss=0.0439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | TrainLoss=0.043998 | ValLoss=6.006734 | ValR2=-0.5753 | ValRMSE=2673569275732.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:34<00:00,  2.18s/it, batch_loss=0.0630, avg_loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | TrainLoss=0.043826 | ValLoss=12.506731 | ValR2=-2.2397 | ValRMSE=3834124576073.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MobileNet-v2 Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆ| 71/71 [02:28<00:00,  2.09s/it, batch_loss=0.0428, avg_loss=0.0506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | TrainLoss=0.050231 | ValLoss=3.458182 | ValR2=-0.0431 | ValRMSE=2175600994111.65\n",
      "\n",
      "MobileNet-v2 summary: R2=-29624668.0000, RMSE=6014017127.54, Training time=1170.88s\n",
      "Saved best_MobileNetv2.pth (elapsed 19.68 min)\n",
      "\n",
      "[3/21] -> ResNet\n",
      "\n",
      "============================================================\n",
      "Starting training for: ResNet\n",
      "============================================================\n",
      "Using existing feature_dimensions: 5\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Brendon/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:12<00:00, 7.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Params: 29590593\n",
      "Resource-intensive architecture -> cycles set to 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [07:12<00:00,  6.09s/it, batch_loss=0.1854, avg_loss=0.3538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | TrainLoss=0.351304 | ValLoss=3.533765 | ValR2=-0.0162 | ValRMSE=2147379291180.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [07:14<00:00,  6.12s/it, batch_loss=0.0503, avg_loss=0.1110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | TrainLoss=0.110827 | ValLoss=135.061645 | ValR2=-36.6005 | ValRMSE=13061984157240.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [06:28<00:00,  5.48s/it, batch_loss=0.0754, avg_loss=0.0668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | TrainLoss=0.066764 | ValLoss=3.268799 | ValR2=-0.0082 | ValRMSE=2138871747944.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [06:34<00:00,  5.56s/it, batch_loss=0.0232, avg_loss=0.0428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | TrainLoss=0.042708 | ValLoss=3.232203 | ValR2=-0.0042 | ValRMSE=2134644047490.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [07:49<00:00,  6.61s/it, batch_loss=0.0606, avg_loss=0.0470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | TrainLoss=0.047010 | ValLoss=3.227207 | ValR2=-0.0040 | ValRMSE=2134363987206.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [08:03<00:00,  6.81s/it, batch_loss=0.0111, avg_loss=0.0370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | TrainLoss=0.036606 | ValLoss=3.226760 | ValR2=-0.0044 | ValRMSE=2134788451875.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [07:22<00:00,  6.23s/it, batch_loss=0.0161, avg_loss=0.0233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | TrainLoss=0.023343 | ValLoss=3.338398 | ValR2=-0.0284 | ValRMSE=2160199970647.31\n",
      "\n",
      "ResNet summary: R2=-57139148.0000, RMSE=8352275063.91, Training time=3252.30s\n",
      "Saved best_ResNet.pth (elapsed 54.92 min)\n",
      "\n",
      "[4/21] -> DenseNet\n",
      "\n",
      "============================================================\n",
      "Starting training for: DenseNet\n",
      "============================================================\n",
      "Using existing feature_dimensions: 5\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\Brendon/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:15<00:00, 2.07MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized. Params: 12451689\n",
      "Resource-intensive architecture -> cycles set to 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DenseNet Epoch 1/7:   0%|                                                    | 0/71 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training DenseNet: mat1 and mat2 shapes cannot be multiplied (16x1029 and 1005x1536)\n",
      "\n",
      "[5/21] -> Xception\n",
      "\n",
      "============================================================\n",
      "Starting training for: Xception\n",
      "============================================================\n",
      "Using existing feature_dimensions: 5\n",
      "Model initialized. Params: 29590593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xception Epoch 1/7:  27%|â–ˆâ–Š     | 19/71 [02:04<05:12,  6.01s/it, batch_loss=0.2960, avg_loss=0.5972]"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 7: Train All Models\n",
    "# -----------------------------\n",
    "model_names = [\n",
    "    'EfficientNet', 'MobileNet-v2', 'ResNet', 'DenseNet', 'Xception',\n",
    "    'Inception-V3', 'GoogleNet', 'VGG', 'Squeeze-and-Excitation',\n",
    "    'Residual-Attention', 'WideResNet', 'Inception-ResNet-v2',\n",
    "    'Inception-V4', 'Competitive-SE', 'HRNetV2', 'FractalNet',\n",
    "    'Highway', 'AlexNet', 'NIN', 'ZFNet', 'CapsuleNet'\n",
    "]\n",
    "\n",
    "trained_models = {}\n",
    "complete_performance_metrics = []\n",
    "\n",
    "# Auto-detect train/val/test loader variables already created above\n",
    "# (Our variables are train_loader, val_loader, test_loader)\n",
    "if 'feature_dimensions' not in globals():\n",
    "    try:\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        globals()['feature_dimensions'] = sample_batch[1].shape[1]\n",
    "        print(f\"Set feature_dimensions = {globals()['feature_dimensions']}\")\n",
    "    except Exception:\n",
    "        # fallback to dataset feature matrix\n",
    "        globals()['feature_dimensions'] = getattr(train_loader.dataset, 'feature_matrix').shape[1]\n",
    "        print(f\"Fallback feature_dimensions = {globals()['feature_dimensions']}\")\n",
    "else:\n",
    "    print(f\"Using existing feature_dimensions = {globals()['feature_dimensions']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ðŸš€ Starting training of {len(model_names)} models on {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, name in enumerate(model_names, start=1):\n",
    "    print(f\"\\n[{i}/{len(model_names)}] -> {name}\")\n",
    "    start = time.time()\n",
    "    try:\n",
    "        model, metrics = execute_model_training(\n",
    "            network_name=name,\n",
    "            training_loader=train_loader,\n",
    "            validation_loader=val_loader,\n",
    "            training_cycles=7,\n",
    "            learning_rate=0.003\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        trained_models[name] = model\n",
    "        complete_performance_metrics.append(metrics)\n",
    "\n",
    "        # Save the final trained model state + metrics\n",
    "        safe_name = name.replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        file_name = f\"best_{safe_name}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'architecture_name': name,\n",
    "            'metrics': metrics,\n",
    "            'feature_dimensions': globals().get('feature_dimensions', None)\n",
    "        }, file_name)\n",
    "        print(f\"Saved {file_name} (elapsed {elapsed/60:.2f} min)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nAll training loops finished.\")\n",
    "print(f\"Successfully trained {len(complete_performance_metrics)} models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 8: Evaluation & Comparison\n",
    "# -----------------------------\n",
    "if len(complete_performance_metrics) == 0:\n",
    "    print(\"No successful trainings recorded.\")\n",
    "    performance_comparison_df = pd.DataFrame(columns=['Architecture','RÂ² Score','RMSE','MSE','MAE','Training Duration (s)'])\n",
    "else:\n",
    "    performance_comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            'Architecture': m['architecture_name'],\n",
    "            'RÂ² Score': m['evaluation_r2'],\n",
    "            'RMSE': m['evaluation_rmse'],\n",
    "            'MSE': m['evaluation_mse'],\n",
    "            'MAE': m['evaluation_mae'],\n",
    "            'Training Duration (s)': m['training_duration']\n",
    "        } for m in complete_performance_metrics\n",
    "    ])\n",
    "    if 'RÂ² Score' in performance_comparison_df.columns and len(performance_comparison_df)>0:\n",
    "        performance_comparison_df = performance_comparison_df.sort_values('RÂ² Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEURAL NETWORK ARCHITECTURE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "if len(performance_comparison_df) > 0:\n",
    "    print(performance_comparison_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No performance data to show.\")\n",
    "\n",
    "# Determine best architecture (if any)\n",
    "if len(performance_comparison_df) > 0:\n",
    "    best_arch = performance_comparison_df.iloc[0]['Architecture']\n",
    "    best_metrics = next((m for m in complete_performance_metrics if m['architecture_name']==best_arch), None)\n",
    "    print(f\"\\nðŸ† Best architecture: {best_arch}\")\n",
    "    if best_metrics:\n",
    "        print(f\"  RÂ²: {best_metrics['evaluation_r2']:.4f}, RMSE: {best_metrics['evaluation_rmse']:.2f}, MAE: {best_metrics['evaluation_mae']:.2f}\")\n",
    "else:\n",
    "    best_arch = None\n",
    "    best_metrics = None\n",
    "    print(\"No best architecture determined.\")\n",
    "\n",
    "# Save comparison CSV\n",
    "performance_comparison_df.to_csv('architecture_performance_comparison.csv', index=False)\n",
    "print(\"Saved architecture_performance_comparison.csv\")\n",
    "\n",
    "# Save best model artifact\n",
    "if best_arch and best_arch in trained_models:\n",
    "    save_obj = {\n",
    "        'architecture': best_arch,\n",
    "        'metrics': best_metrics,\n",
    "        'model_state_dict': trained_models[best_arch].state_dict(),\n",
    "        'complete_metrics': complete_performance_metrics\n",
    "    }\n",
    "    torch.save(save_obj, 'optimal_property_valuation_network.pth')\n",
    "    print(\"Saved optimal_property_valuation_network.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013deb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# STEP 9: Visualization\n",
    "# -----------------------------\n",
    "if len(performance_comparison_df) > 0:\n",
    "    fig, axes = plt.subplots(2,2, figsize=(16,12))\n",
    "    ax1 = axes[0,0]\n",
    "    ax1.barh(performance_comparison_df['Architecture'], performance_comparison_df['RÂ² Score'])\n",
    "    ax1.set_title('RÂ² Score by Architecture')\n",
    "    ax1.set_xlabel('RÂ² Score')\n",
    "\n",
    "    ax2 = axes[0,1]\n",
    "    ax2.barh(performance_comparison_df['Architecture'], performance_comparison_df['RMSE'])\n",
    "    ax2.set_title('RMSE by Architecture')\n",
    "    ax2.set_xlabel('RMSE (USD)')\n",
    "\n",
    "    ax3 = axes[1,0]\n",
    "    ax3.barh(performance_comparison_df['Architecture'], performance_comparison_df['Training Duration (s)'])\n",
    "    ax3.set_title('Training Duration (s)')\n",
    "    ax3.set_xlabel('Seconds')\n",
    "\n",
    "    ax4 = axes[1,1]\n",
    "    scatter = ax4.scatter(performance_comparison_df['RMSE'], performance_comparison_df['RÂ² Score'],\n",
    "                          s=120, c=performance_comparison_df['Training Duration (s)'], cmap='plasma')\n",
    "    ax4.set_xlabel('RMSE')\n",
    "    ax4.set_ylabel('RÂ² Score')\n",
    "    plt.colorbar(scatter, ax=ax4, label='Training Duration (s)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_img = 'architecture_performance_comparison_visualization.png'\n",
    "    plt.savefig(out_img, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved {out_img}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Pipeline complete. Artifacts produced:\")\n",
    "print(\" - architecture_performance_comparison.csv\")\n",
    "print(\" - architecture_performance_comparison_visualization.png\")\n",
    "print(\" - best_<architecture>.pth files for successfully trained models\")\n",
    "print(\" - optimal_property_valuation_network.pth (if a best model exists)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8dc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabeeaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bfe98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d6148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef0e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
